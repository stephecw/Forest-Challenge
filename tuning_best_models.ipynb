{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation hyperparamètres des modèles prometteurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va optimiser les hyperparamètres des modèles prometteurs que nous avons identifiés dans le notebook précédent. \n",
    "\n",
    "On va utiliser des méthodes de recherche d'hyperparamètres par validation croisée pour éviter le surapprentissage.\n",
    "\n",
    "On va utiliser les méthodes bayésiennes pour optimiser les hyperparamètres des modèles de manière intelligente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# On sélectionne les colonnes d'entrée (features) et la cible\n",
    "X = data.drop(columns=['Cover_Type'])\n",
    "y = data['Cover_Type']\n",
    "\n",
    "# On sélectionne les colonnes d'entrée (features) et la cible\n",
    "X = data.drop(columns=['Cover_Type'])\n",
    "y = data['Cover_Type']\n",
    "\n",
    "# On divise les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# On normalise les données continues \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "continuous_columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', \n",
    "                      'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', \n",
    "                      'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# On applique le scaler uniquement sur les colonnes continues\n",
    "X_train[continuous_columns] = scaler.fit_transform(X_train[continuous_columns])\n",
    "X_test[continuous_columns] = scaler.transform(X_test[continuous_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [27:22<00:00, 32.86s/trial, best loss: -0.8746220710506425]  \n",
      "Meilleurs paramètres Random Forest : {'max_depth': 4, 'max_features': 0, 'min_samples_leaf': 0, 'min_samples_split': 0, 'n_estimators': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# On définit l'espace de recherche pour Random Forest\n",
    "space_rf = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [5, 10, 15, 20, None]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4]),\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'log2'])  # Correction : 'auto' remplacé par 'sqrt'\n",
    "}\n",
    "\n",
    "# On crée la fonction objectif pour Random Forest\n",
    "def objective_rf(params):\n",
    "    # On initialise le modèle avec les paramètres proposés\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    # On effectue une validation croisée pour évaluer la performance\n",
    "    score = cross_val_score(model, X_train, y_train, cv=7, scoring='accuracy').mean()\n",
    "    # On retourne la perte (score négatif) car Hyperopt minimise cette fonction\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# On lance Hyperopt pour optimiser les hyperparamètres de Random Forest\n",
    "trials_rf = Trials()\n",
    "best_params_rf = fmin(fn=objective_rf, space=space_rf, algo=tpe.suggest, max_evals=50, trials=trials_rf)\n",
    "print(\"Meilleurs paramètres Random Forest :\", best_params_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste ensuite le modèle avec les hyperparamètres optimisés sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision Random Forest : 0.8763227513227513\n",
      "Rapport de classification Random Forest :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.78      0.79       648\n",
      "           2       0.80      0.71      0.75       648\n",
      "           3       0.85      0.88      0.87       648\n",
      "           4       0.95      0.98      0.96       648\n",
      "           5       0.92      0.95      0.93       648\n",
      "           6       0.88      0.88      0.88       648\n",
      "           7       0.93      0.95      0.94       648\n",
      "\n",
      "    accuracy                           0.88      4536\n",
      "   macro avg       0.87      0.88      0.87      4536\n",
      "weighted avg       0.87      0.88      0.87      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Traduction des indices en valeurs concrètes\n",
    "best_rf_params = {\n",
    "    'max_depth': [5, 10, 15, 20, None][4],           \n",
    "    'max_features': ['sqrt', 'log2'][0],             \n",
    "    'min_samples_leaf': [1, 2, 4][0],                \n",
    "    'min_samples_split': [2, 5, 10][0],              \n",
    "    'n_estimators': [100, 200, 300, 400, 500][2]     \n",
    "}\n",
    "\n",
    "# Initialiser le modèle avec les valeurs concrètes\n",
    "rf_model = RandomForestClassifier(**best_rf_params, random_state=42)\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Précision Random Forest :\", accuracy_rf)\n",
    "print(\"Rapport de classification Random Forest :\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [24:26<00:00, 29.34s/trial, best loss: -0.8828420256991685]   \n",
      "Meilleurs paramètres XGBoost : {'colsample_bytree': 0.9284956459967804, 'gamma': 0.04924511492281908, 'learning_rate': 0.24122412239323296, 'max_depth': 2, 'n_estimators': 1, 'subsample': 0.9833984902058166}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_adj = y_train - 1\n",
    "y_test_adj = y_test - 1\n",
    "\n",
    "# On définit l'espace de recherche pour XGBoost\n",
    "space_xgb = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [3, 6, 9, 12]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0, 0.5)\n",
    "}\n",
    "\n",
    "# On crée la fonction objectif pour XGBoost\n",
    "def objective_xgb(params):\n",
    "    # On initialise XGBoost avec les paramètres proposés\n",
    "    model = xgb.XGBClassifier(objective='multi:softmax', num_class=7, **params, random_state=42)\n",
    "    # On utilise une validation croisée pour calculer la précision\n",
    "    score = cross_val_score(model, X_train, y_train_adj, cv=7, scoring='accuracy').mean()\n",
    "    # On retourne la perte (score négatif) car Hyperopt minimise cette fonction\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# On lance l'optimisation avec Hyperopt pour XGBoost\n",
    "trials_xgb = Trials()\n",
    "best_params_xgb = fmin(fn=objective_xgb, space=space_xgb, algo=tpe.suggest, max_evals=50, trials=trials_xgb)\n",
    "print(\"Meilleurs paramètres XGBoost :\", best_params_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste ensuite le modèle avec les hyperparamètres optimisés sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle XGBoost : 0.8862433862433863\n",
      "Rapport de classification XGBoost :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80       648\n",
      "           1       0.80      0.73      0.77       648\n",
      "           2       0.88      0.90      0.89       648\n",
      "           3       0.96      0.97      0.97       648\n",
      "           4       0.91      0.95      0.93       648\n",
      "           5       0.89      0.92      0.90       648\n",
      "           6       0.94      0.94      0.94       648\n",
      "\n",
      "    accuracy                           0.89      4536\n",
      "   macro avg       0.88      0.89      0.89      4536\n",
      "weighted avg       0.88      0.89      0.89      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On utilise les meilleurs hyperparamètres trouvés par Hyperopt\n",
    "best_xgb_params = {\n",
    "    'colsample_bytree': 0.9284956459967804,\n",
    "    'gamma': 0.04924511492281908,\n",
    "    'learning_rate': 0.24122412239323296,\n",
    "    'max_depth': 9,\n",
    "    'n_estimators': [100, 200, 300, 400, 500][2],  \n",
    "    'subsample': 0.9833984902058166\n",
    "}\n",
    "\n",
    "# Initialiser le modèle XGBoost avec les paramètres optimaux\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=7, **best_xgb_params, random_state=42)\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "xgb_model.fit(X_train, y_train_adj)\n",
    "\n",
    "# Prédire les classes sur l'ensemble de test\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy_xgb = accuracy_score(y_test_adj, y_pred_xgb)\n",
    "print(\"Précision du modèle XGBoost :\", accuracy_xgb)\n",
    "print(\"Rapport de classification XGBoost :\\n\", classification_report(y_test_adj, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [53:20<00:00, 64.01s/trial, best loss: -0.8912509448223733]  \n",
      "Meilleurs paramètres LightGBM : {'colsample_bytree': 0.9488048655607615, 'learning_rate': 0.09224159033088929, 'max_depth': 2, 'n_estimators': 2, 'num_leaves': 3, 'subsample': 0.5190158468602316}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# On définit l'espace de recherche pour LightGBM\n",
    "space_lgb = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [5, 10, 15, 20, -1]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'num_leaves': hp.choice('num_leaves', [31, 50, 70, 100]),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0)\n",
    "}\n",
    "\n",
    "# On crée la fonction objectif pour LightGBM\n",
    "def objective_lgb(params):\n",
    "    # On initialise LightGBM avec les paramètres proposés\n",
    "    model = lgb.LGBMClassifier(objective='multiclass', **params, verbosity = -1,random_state=42)\n",
    "    # On utilise la validation croisée pour évaluer la précision\n",
    "    score = cross_val_score(model, X_train, y_train, cv=7, scoring='accuracy').mean()\n",
    "    # On retourne la perte (score négatif) car Hyperopt minimise cette fonction\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# On lance l'optimisation avec Hyperopt pour LightGBM\n",
    "trials_lgb = Trials()\n",
    "best_params_lgb = fmin(fn=objective_lgb, space=space_lgb, algo=tpe.suggest, max_evals=50, trials=trials_lgb)\n",
    "print(\"Meilleurs paramètres LightGBM :\", best_params_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste ensuite le modèle avec les hyperparamètres optimisés sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle LightGBM : 0.8952821869488536\n",
      "Rapport de classification LightGBM :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.81      0.81       648\n",
      "           2       0.82      0.75      0.78       648\n",
      "           3       0.89      0.90      0.89       648\n",
      "           4       0.96      0.98      0.97       648\n",
      "           5       0.93      0.96      0.95       648\n",
      "           6       0.90      0.92      0.91       648\n",
      "           7       0.94      0.96      0.95       648\n",
      "\n",
      "    accuracy                           0.90      4536\n",
      "   macro avg       0.89      0.90      0.89      4536\n",
      "weighted avg       0.89      0.90      0.89      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Meilleurs paramètres de LightGBM (après avoir traduit les indices en valeurs)\n",
    "best_lgb_params = {\n",
    "    'colsample_bytree': 0.9488048655607615,\n",
    "    'learning_rate': 0.09224159033088929,\n",
    "    'max_depth': 15,\n",
    "    'n_estimators': [100, 200, 300, 400, 500][2],  \n",
    "    'num_leaves': [31, 50, 70, 100][3],             \n",
    "    'subsample': 0.5190158468602316,\n",
    "    'verbosity': -1  # Désactive les avertissements pendant l'entraînement\n",
    "}\n",
    "\n",
    "# Initialiser le modèle LightGBM avec les paramètres optimisés\n",
    "lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=7, **best_lgb_params, random_state=42)\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les classes sur l'ensemble de test\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "print(\"Précision du modèle LightGBM :\", accuracy_lgb)\n",
    "print(\"Rapport de classification LightGBM :\\n\", classification_report(y_test, y_pred_lgb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
