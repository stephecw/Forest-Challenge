{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning des modèles en enlevant Hillshade_9am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# On sélectionne les colonnes d'entrée (features) et la cible\n",
    "X = data.drop(columns=['Cover_Type'])\n",
    "y = data['Cover_Type']\n",
    "\n",
    "# On sélectionne les colonnes d'entrée (features) et la cible\n",
    "X = data.drop(columns=['Cover_Type'])\n",
    "y = data['Cover_Type']\n",
    "\n",
    "# On divise les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# On normalise les données continues \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "continuous_columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', \n",
    "                      'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', \n",
    "                      'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# On applique le scaler uniquement sur les colonnes continues\n",
    "#X_train[continuous_columns] = scaler.fit_transform(X_train[continuous_columns])\n",
    "#X_test[continuous_columns] = scaler.transform(X_test[continuous_columns])\n",
    "\n",
    "X_train_hill = X_train.drop(columns=['Hillshade_9am'])\n",
    "X_test_hill = X_test.drop(columns=['Hillshade_9am'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [14:28<00:00, 17.37s/trial, best loss: -0.8778344671201813]\n",
      "Meilleurs paramètres Random Forest : {'max_depth': 4, 'max_features': 0, 'min_samples_leaf': 0, 'min_samples_split': 0, 'n_estimators': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# On définit l'espace de recherche pour Random Forest\n",
    "space_rf = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [5, 10, 15, 20, None]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4]),\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'log2'])  # Correction : 'auto' remplacé par 'sqrt'\n",
    "}\n",
    "\n",
    "# On crée la fonction objectif pour Random Forest\n",
    "def objective_rf(params):\n",
    "    # On initialise le modèle avec les paramètres proposés\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    # On effectue une validation croisée pour évaluer la performance\n",
    "    score = cross_val_score(model, X_train_hill, y_train, cv=7, scoring='accuracy').mean()\n",
    "    # On retourne la perte (score négatif) car Hyperopt minimise cette fonction\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# On lance Hyperopt pour optimiser les hyperparamètres de Random Forest\n",
    "trials_rf = Trials()\n",
    "best_params_rf = fmin(fn=objective_rf, space=space_rf, algo=tpe.suggest, max_evals=50, trials=trials_rf)\n",
    "print(\"Meilleurs paramètres Random Forest :\", best_params_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision Random Forest : 0.8847001763668431\n",
      "Rapport de classification Random Forest :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.79      0.80       648\n",
      "           2       0.82      0.73      0.77       648\n",
      "           3       0.86      0.90      0.88       648\n",
      "           4       0.95      0.98      0.96       648\n",
      "           5       0.92      0.96      0.94       648\n",
      "           6       0.89      0.88      0.89       648\n",
      "           7       0.93      0.95      0.94       648\n",
      "\n",
      "    accuracy                           0.88      4536\n",
      "   macro avg       0.88      0.88      0.88      4536\n",
      "weighted avg       0.88      0.88      0.88      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Traduction des indices en valeurs concrètes\n",
    "best_rf_params = {\n",
    "    'max_depth': [5, 10, 15, 20, None][4],           \n",
    "    'max_features': ['sqrt', 'log2'][0],             \n",
    "    'min_samples_leaf': [1, 2, 4][0],                \n",
    "    'min_samples_split': [2, 5, 10][0],              \n",
    "    'n_estimators': [100, 200, 300, 400, 500][2]     \n",
    "}\n",
    "\n",
    "# Initialiser le modèle avec les valeurs concrètes\n",
    "rf_model = RandomForestClassifier(**best_rf_params, random_state=42)\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "rf_model.fit(X_train_hill, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred_rf = rf_model.predict(X_test_hill)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Précision Random Forest :\", accuracy_rf)\n",
    "print(\"Rapport de classification Random Forest :\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_adj = y_train - 1\n",
    "y_test_adj = y_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [12:22<00:00, 14.86s/trial, best loss: -0.8839758125472411]\n",
      "Meilleurs paramètres XGBoost : {'colsample_bytree': 0.9935889419671114, 'gamma': 0.012860005064671678, 'learning_rate': 0.11209385123371478, 'max_depth': 2, 'n_estimators': 1, 'subsample': 0.7078887451962781}\n"
     ]
    }
   ],
   "source": [
    "# On définit l'espace de recherche pour XGBoost\n",
    "space_xgb = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [3, 6, 9, 12]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0, 0.5)\n",
    "}\n",
    "\n",
    "# On crée la fonction objectif pour XGBoost\n",
    "def objective_xgb(params):\n",
    "    # On initialise XGBoost avec les paramètres proposés\n",
    "    model = xgb.XGBClassifier(objective='multi:softmax', num_class=7, **params, random_state=42)\n",
    "    # On utilise une validation croisée pour calculer la précision\n",
    "    score = cross_val_score(model, X_train_hill, y_train_adj, cv=7, scoring='accuracy').mean()\n",
    "    # On retourne la perte (score négatif) car Hyperopt minimise cette fonction\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# On lance l'optimisation avec Hyperopt pour XGBoost\n",
    "trials_xgb = Trials()\n",
    "best_params_xgb = fmin(fn=objective_xgb, space=space_xgb, algo=tpe.suggest, max_evals=50, trials=trials_xgb)\n",
    "print(\"Meilleurs paramètres XGBoost :\", best_params_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle XGBoost : 0.88668430335097\n",
      "Rapport de classification XGBoost :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.79       648\n",
      "           1       0.79      0.72      0.76       648\n",
      "           2       0.89      0.90      0.89       648\n",
      "           3       0.95      0.98      0.97       648\n",
      "           4       0.92      0.95      0.93       648\n",
      "           5       0.90      0.91      0.91       648\n",
      "           6       0.94      0.96      0.95       648\n",
      "\n",
      "    accuracy                           0.89      4536\n",
      "   macro avg       0.88      0.89      0.89      4536\n",
      "weighted avg       0.88      0.89      0.89      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# On utilise les meilleurs hyperparamètres trouvés par Hyperopt\n",
    "best_xgb_params = {\n",
    "    'colsample_bytree': 0.9935889419671114,\n",
    "    'gamma': 0.012860005064671678,\n",
    "    'learning_rate': 0.11209385123371478,\n",
    "    'max_depth': 9,\n",
    "    'n_estimators': [100, 200, 300, 400, 500][1],  \n",
    "    'subsample': 0.7078887451962781\n",
    "}\n",
    "\n",
    "# Initialiser le modèle XGBoost avec les paramètres optimaux\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=7, **best_xgb_params, random_state=42)\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "xgb_model.fit(X_train_hill, y_train_adj)\n",
    "\n",
    "# Prédire les classes sur l'ensemble de test\n",
    "y_pred_xgb = xgb_model.predict(X_test_hill)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy_xgb = accuracy_score(y_test_adj, y_pred_xgb)\n",
    "print(\"Précision du modèle XGBoost :\", accuracy_xgb)\n",
    "print(\"Rapport de classification XGBoost :\\n\", classification_report(y_test_adj, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [49:50<00:00, 59.81s/trial, best loss: -0.8915343915343915]   \n",
      "Meilleurs paramètres LightGBM : {'colsample_bytree': 0.6284626649933062, 'learning_rate': 0.08427513495693706, 'max_depth': 2, 'n_estimators': 1, 'num_leaves': 3, 'subsample': 0.6894988308344379}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# On définit l'espace de recherche pour LightGBM\n",
    "space_lgb = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [5, 10, 15, 20, -1]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'num_leaves': hp.choice('num_leaves', [31, 50, 70, 100]),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0)\n",
    "}\n",
    "\n",
    "# On crée la fonction objectif pour LightGBM\n",
    "def objective_lgb(params):\n",
    "    # On initialise LightGBM avec les paramètres proposés\n",
    "    model = lgb.LGBMClassifier(objective='multiclass', **params, verbosity = -1,random_state=42)\n",
    "    # On utilise la validation croisée pour évaluer la précision\n",
    "    score = cross_val_score(model, X_train_hill, y_train, cv=7, scoring='accuracy').mean()\n",
    "    # On retourne la perte (score négatif) car Hyperopt minimise cette fonction\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# On lance l'optimisation avec Hyperopt pour LightGBM\n",
    "trials_lgb = Trials()\n",
    "best_params_lgb = fmin(fn=objective_lgb, space=space_lgb, algo=tpe.suggest, max_evals=50, trials=trials_lgb)\n",
    "print(\"Meilleurs paramètres LightGBM :\", best_params_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle LightGBM : 0.8941798941798942\n",
      "Rapport de classification LightGBM :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.80      0.81       648\n",
      "           2       0.81      0.75      0.78       648\n",
      "           3       0.89      0.90      0.89       648\n",
      "           4       0.96      0.98      0.97       648\n",
      "           5       0.93      0.96      0.94       648\n",
      "           6       0.90      0.92      0.91       648\n",
      "           7       0.94      0.95      0.95       648\n",
      "\n",
      "    accuracy                           0.89      4536\n",
      "   macro avg       0.89      0.89      0.89      4536\n",
      "weighted avg       0.89      0.89      0.89      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Meilleurs paramètres de LightGBM (après avoir traduit les indices en valeurs)\n",
    "best_lgb_params = {\n",
    "    'colsample_bytree': 0.6284626649933062,\n",
    "    'learning_rate': 0.08427513495693706,\n",
    "    'max_depth': 15,\n",
    "    'n_estimators': [100, 200, 300, 400, 500][1],  \n",
    "    'num_leaves': [31, 50, 70, 100][3],             \n",
    "    'subsample': 0.6894988308344379,\n",
    "    'verbosity': -1  # Désactive les avertissements pendant l'entraînement\n",
    "}\n",
    "\n",
    "# Initialiser le modèle LightGBM avec les paramètres optimisés\n",
    "lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=7, **best_lgb_params, random_state=42)\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "lgb_model.fit(X_train_hill, y_train)\n",
    "\n",
    "# Prédire les classes sur l'ensemble de test\n",
    "y_pred_lgb = lgb_model.predict(X_test_hill)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "print(\"Précision du modèle LightGBM :\", accuracy_lgb)\n",
    "print(\"Rapport de classification LightGBM :\\n\", classification_report(y_test, y_pred_lgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [15:32<00:00, 18.65s/trial, best loss: -0.881708238851096] \n",
      "Meilleurs paramètres ExtraTreesClassifier : {'max_depth': 0, 'max_features': 2, 'min_samples_leaf': 0, 'min_samples_split': 0, 'n_estimators': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Définir l'espace de recherche pour ExtraTreesClassifier\n",
    "space_et = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),  # Choix pour le nombre d'arbres\n",
    "    'max_depth': hp.choice('max_depth', [None, 10, 20, 30, 40]),           # Profondeur maximale\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),       # Nombre minimum d'échantillons pour diviser\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4]),          # Nombre minimum d'échantillons dans une feuille\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'log2', None])      # Nombre de caractéristiques pour chaque division\n",
    "}\n",
    "\n",
    "# Fonction objectif pour Hyperopt\n",
    "def objective_et(params):\n",
    "    # Initialiser ExtraTreesClassifier avec les paramètres testés\n",
    "    model = ExtraTreesClassifier(**params, random_state=42)\n",
    "    # Utiliser la validation croisée pour évaluer la précision\n",
    "    score = cross_val_score(model, X_train_hill, y_train, cv=7, scoring='accuracy').mean()\n",
    "    # Retourner la perte (score négatif) car Hyperopt minimise cette fonction\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Lancer l'optimisation avec Hyperopt pour ExtraTreesClassifier\n",
    "trials_et = Trials()\n",
    "best_params_et = fmin(\n",
    "    fn=objective_et,               # Fonction objectif\n",
    "    space=space_et,                # Espace de recherche\n",
    "    algo=tpe.suggest,              # Algorithme bayésien pour l'optimisation\n",
    "    max_evals=50,                  # Nombre d'itérations de recherche\n",
    "    trials=trials_et,              # Historique des essais\n",
    "    rstate=np.random.default_rng(42)  # Fixer un seed pour la reproductibilité\n",
    ")\n",
    "\n",
    "# Afficher les meilleurs paramètres trouvés\n",
    "print(\"Meilleurs paramètres ExtraTreesClassifier :\", best_params_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle ExtraTrees : 0.8835978835978836\n",
      "Rapport de classification ExtraTrees :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.79      0.79       648\n",
      "           2       0.80      0.72      0.76       648\n",
      "           3       0.88      0.90      0.89       648\n",
      "           4       0.95      0.98      0.97       648\n",
      "           5       0.92      0.95      0.94       648\n",
      "           6       0.89      0.89      0.89       648\n",
      "           7       0.94      0.95      0.94       648\n",
      "\n",
      "    accuracy                           0.88      4536\n",
      "   macro avg       0.88      0.88      0.88      4536\n",
      "weighted avg       0.88      0.88      0.88      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Meilleurs paramètres de ExtraTreesClassifier\n",
    "best_et_params = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500][2],  \n",
    "    'max_depth': [None, 10, 20, 30, 40][0],           \n",
    "    'min_samples_split': [2, 5, 10][0],       \n",
    "    'min_samples_leaf': [1, 2, 4][0],          \n",
    "    'max_features': ['sqrt', 'log2', None][2]\n",
    "}\n",
    "\n",
    "# Initialiser le modèle LightGBM avec les paramètres optimisés\n",
    "et_model = ExtraTreesClassifier(**best_et_params, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "et_model.fit(X_train_hill, y_train)\n",
    "\n",
    "# Prédire les classes sur l'ensemble de test\n",
    "y_pred_et = et_model.predict(X_test_hill)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy_et = accuracy_score(y_test, y_pred_et)\n",
    "print(\"Précision du modèle ExtraTrees :\", accuracy_et)\n",
    "print(\"Rapport de classification ExtraTrees :\\n\", classification_report(y_test, y_pred_et))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [9:10:03<00:00, 660.07s/trial, best loss: -0.8909674981103554]    \n",
      "Meilleurs paramètres GradientBoostingClassifier : {'learning_rate': 0.11317932139355968, 'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 3, 'subsample': 0.8011279046777869}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Définir l'espace de recherche pour GradientBoostingClassifier\n",
    "space_gb = {\n",
    "    'n_estimators': hp.choice('n_estimators', [ 100, 200, 300, 400]),     # Nombre d'arbres\n",
    "    'max_depth': hp.choice('max_depth', [5, 7, 10, 12]),                  # Profondeur maximale\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.05, 0.3),              # Taux d'apprentissage\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),      # Nombre minimum d'échantillons pour diviser\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4]),         # Nombre minimum d'échantillons dans une feuille\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0)                        # Fraction des échantillons pour chaque arbre\n",
    "}\n",
    "\n",
    "# Fonction objectif pour Hyperopt\n",
    "def objective_gb(params):\n",
    "    # Initialiser GradientBoostingClassifier avec les paramètres proposés\n",
    "    model = GradientBoostingClassifier(**params, random_state=42)\n",
    "    # Utiliser une validation croisée pour calculer la précision moyenne\n",
    "    score = cross_val_score(model, X_train_hill, y_train, cv=7, scoring='accuracy').mean()\n",
    "    # Retourner la perte (score négatif) car Hyperopt minimise cette fonction\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Lancer l'optimisation avec Hyperopt pour GradientBoostingClassifier\n",
    "trials_gb = Trials()\n",
    "best_params_gb = fmin(\n",
    "    fn=objective_gb,               # Fonction objectif\n",
    "    space=space_gb,                # Espace de recherche\n",
    "    algo=tpe.suggest,              # Algorithme bayésien pour l'optimisation\n",
    "    max_evals=50,                  # Nombre d'itérations de recherche\n",
    "    trials=trials_gb,              # Historique des essais\n",
    "    rstate=np.random.default_rng(42)  # Fixer un seed pour la reproductibilité\n",
    ")\n",
    "\n",
    "# Afficher les meilleurs paramètres trouvés\n",
    "print(\"Meilleurs paramètres GradientBoostingClassifier :\", best_params_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle GradientBoosting : 0.8932980599647267\n",
      "Rapport de classification GradientBoosting :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.81      0.81       648\n",
      "           2       0.82      0.74      0.78       648\n",
      "           3       0.88      0.90      0.89       648\n",
      "           4       0.96      0.98      0.97       648\n",
      "           5       0.93      0.96      0.94       648\n",
      "           6       0.90      0.91      0.90       648\n",
      "           7       0.94      0.95      0.95       648\n",
      "\n",
      "    accuracy                           0.89      4536\n",
      "   macro avg       0.89      0.89      0.89      4536\n",
      "weighted avg       0.89      0.89      0.89      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Meilleurs paramètres de GradientBoostingClassifier\n",
    "best_gb_params = {\n",
    "    'n_estimators': 300,  \n",
    "    'max_depth': 16,#[5, 7, 10, 12][3],           \n",
    "    'learning_rate': 0.11782851482424293,  \n",
    "    'min_samples_split': 5,       \n",
    "    'min_samples_leaf': 6,         \n",
    "    'subsample': 0.7409699669018692  \n",
    "}\n",
    "\n",
    "# Initialiser le modèle GradientBoostingClassifier avec les paramètres optimisés\n",
    "gb_model = GradientBoostingClassifier(**best_gb_params, random_state=42)\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "gb_model.fit(X_train_hill, y_train)\n",
    "\n",
    "# Prédire les classes sur l'ensemble de test\n",
    "y_pred_gb = gb_model.predict(X_test_hill)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(\"Précision du modèle GradientBoosting :\", accuracy_gb)\n",
    "print(\"Rapport de classification GradientBoosting :\\n\", classification_report(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 12:00:46,821] Using an existing study with name 'gb_optimization' instead of creating a new one.\n",
      "[I 2024-12-07 12:20:36,350] Trial 9 finished with value: 0.8881330309901739 and parameters: {'n_estimators': 400, 'max_depth': 10, 'learning_rate': 0.0988326219059468, 'min_samples_split': 13, 'min_samples_leaf': 4, 'subsample': 0.5700002735264409}. Best is trial 9 with value: 0.8881330309901739.\n",
      "[I 2024-12-07 12:21:57,753] Trial 15 finished with value: 0.8897392290249434 and parameters: {'n_estimators': 500, 'max_depth': 14, 'learning_rate': 0.13511644147178914, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.7894424176815351}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 12:22:35,429] Trial 11 finished with value: 0.888416477702192 and parameters: {'n_estimators': 450, 'max_depth': 14, 'learning_rate': 0.1339486825543792, 'min_samples_split': 5, 'min_samples_leaf': 4, 'subsample': 0.8689900170747198}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 12:22:52,083] Trial 10 finished with value: 0.8856764928193499 and parameters: {'n_estimators': 500, 'max_depth': 12, 'learning_rate': 0.13239508060243255, 'min_samples_split': 13, 'min_samples_leaf': 2, 'subsample': 0.920502212860723}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 12:25:03,639] Trial 13 finished with value: 0.8868102796674225 and parameters: {'n_estimators': 500, 'max_depth': 14, 'learning_rate': 0.13265420022940067, 'min_samples_split': 5, 'min_samples_leaf': 2, 'subsample': 0.9265817698846379}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 12:25:49,477] Trial 8 finished with value: 0.8874716553287981 and parameters: {'n_estimators': 300, 'max_depth': 12, 'learning_rate': 0.05973735023350821, 'min_samples_split': 10, 'min_samples_leaf': 2, 'subsample': 0.7270170992018985}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 12:26:08,374] Trial 12 finished with value: 0.8869047619047619 and parameters: {'n_estimators': 300, 'max_depth': 12, 'learning_rate': 0.11006510480809586, 'min_samples_split': 5, 'min_samples_leaf': 4, 'subsample': 0.9421921196407297}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 12:28:09,364] Trial 14 finished with value: 0.8856764928193499 and parameters: {'n_estimators': 400, 'max_depth': 10, 'learning_rate': 0.09995368829958015, 'min_samples_split': 10, 'min_samples_leaf': 4, 'subsample': 0.9561930447846541}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 12:39:10,235] Trial 17 finished with value: 0.8886054421768707 and parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.12972598602344676, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.6513141429030945}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 12:42:12,752] Trial 16 finished with value: 0.8874716553287981 and parameters: {'n_estimators': 450, 'max_depth': 12, 'learning_rate': 0.13987462591937613, 'min_samples_split': 10, 'min_samples_leaf': 6, 'subsample': 0.522529481912489}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 12:47:34,771] Trial 20 finished with value: 0.8878495842781556 and parameters: {'n_estimators': 400, 'max_depth': 10, 'learning_rate': 0.07182079514681264, 'min_samples_split': 13, 'min_samples_leaf': 4, 'subsample': 0.6076997349314642}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 12:56:13,596] Trial 23 finished with value: 0.8866213151927438 and parameters: {'n_estimators': 400, 'max_depth': 10, 'learning_rate': 0.08018131003308462, 'min_samples_split': 10, 'min_samples_leaf': 2, 'subsample': 0.6197013886235578}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 12:57:48,936] Trial 21 finished with value: 0.887660619803477 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.0883102813661515, 'min_samples_split': 10, 'min_samples_leaf': 6, 'subsample': 0.5616130675415162}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 13:03:41,596] Trial 19 finished with value: 0.8852040816326531 and parameters: {'n_estimators': 500, 'max_depth': 14, 'learning_rate': 0.07268819648821168, 'min_samples_split': 10, 'min_samples_leaf': 4, 'subsample': 0.5139882833252422}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 13:06:28,447] Trial 18 finished with value: 0.889644746787604 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.0527632294165753, 'min_samples_split': 5, 'min_samples_leaf': 4, 'subsample': 0.6656057513613025}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 13:07:08,187] Trial 22 finished with value: 0.8875661375661377 and parameters: {'n_estimators': 450, 'max_depth': 12, 'learning_rate': 0.05557298488159662, 'min_samples_split': 10, 'min_samples_leaf': 2, 'subsample': 0.7007396972070055}. Best is trial 15 with value: 0.8897392290249434.\n",
      "[I 2024-12-07 13:36:07,251] Trial 26 finished with value: 0.8910619803476945 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.11691059587495828, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.7443672655314026}. Best is trial 26 with value: 0.8910619803476945.\n",
      "[I 2024-12-07 13:39:13,837] Trial 24 finished with value: 0.8842592592592593 and parameters: {'n_estimators': 400, 'max_depth': 14, 'learning_rate': 0.0757549882562508, 'min_samples_split': 5, 'min_samples_leaf': 2, 'subsample': 0.538051293674999}. Best is trial 26 with value: 0.8910619803476945.\n",
      "[I 2024-12-07 13:40:24,469] Trial 27 finished with value: 0.889928193499622 and parameters: {'n_estimators': 500, 'max_depth': 16, 'learning_rate': 0.11747590140997573, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.75367587443387}. Best is trial 26 with value: 0.8910619803476945.\n",
      "[I 2024-12-07 13:40:58,686] Trial 28 finished with value: 0.8877551020408164 and parameters: {'n_estimators': 500, 'max_depth': 14, 'learning_rate': 0.11785579593672038, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.7403940992229504}. Best is trial 26 with value: 0.8910619803476945.\n",
      "[I 2024-12-07 13:46:00,907] Trial 25 finished with value: 0.8908730158730158 and parameters: {'n_estimators': 500, 'max_depth': 16, 'learning_rate': 0.07537448569367833, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.7887245499725888}. Best is trial 26 with value: 0.8910619803476945.\n",
      "[I 2024-12-07 13:47:44,783] Trial 29 finished with value: 0.8921012849584279 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.11782851482424293, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.7409699669018692}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 13:52:34,079] Trial 31 finished with value: 0.8893613000755859 and parameters: {'n_estimators': 500, 'max_depth': 16, 'learning_rate': 0.11482058248656432, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.7839852045156235}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:04:55,878] Trial 32 finished with value: 0.8891723356009071 and parameters: {'n_estimators': 500, 'max_depth': 16, 'learning_rate': 0.11742802414236483, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.7918986174516137}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:08:05,063] Trial 33 finished with value: 0.8901171579743009 and parameters: {'n_estimators': 500, 'max_depth': 16, 'learning_rate': 0.11537905178857462, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.8000769937849318}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:08:37,085] Trial 35 finished with value: 0.8905895691609979 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.11803168650561441, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.8284064600296533}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:08:46,317] Trial 34 finished with value: 0.8909674981103554 and parameters: {'n_estimators': 500, 'max_depth': 16, 'learning_rate': 0.11738951315598885, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.8140594980622567}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:13:29,612] Trial 36 finished with value: 0.8907785336356764 and parameters: {'n_estimators': 500, 'max_depth': 16, 'learning_rate': 0.11711074883649664, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.810125406904572}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:15:00,403] Trial 37 finished with value: 0.8904950869236584 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.11315301143195111, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.8307139466306354}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:20:57,771] Trial 30 finished with value: 0.889928193499622 and parameters: {'n_estimators': 500, 'max_depth': 16, 'learning_rate': 0.05132108603757735, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.7959345621955187}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:24:03,368] Trial 39 finished with value: 0.8889833711262283 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.14747789084991508, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.883515389884081}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:25:06,519] Trial 38 finished with value: 0.8912509448223734 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.0918537981157371, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.8390783845298445}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:27:07,628] Trial 40 finished with value: 0.8890778533635677 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.14809275003892677, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.8602755931186424}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:27:32,063] Trial 41 finished with value: 0.8921012849584279 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.14711833770181476, 'min_samples_split': 13, 'min_samples_leaf': 6, 'subsample': 0.8704132405703561}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:28:05,655] Trial 42 finished with value: 0.8902116402116401 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.1465875866081032, 'min_samples_split': 13, 'min_samples_leaf': 6, 'subsample': 0.8395784882383202}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:31:55,200] Trial 43 finished with value: 0.8900226757369616 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.14993563674351643, 'min_samples_split': 13, 'min_samples_leaf': 6, 'subsample': 0.8561487743069858}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:33:53,801] Trial 44 finished with value: 0.8900226757369615 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.14661999042940532, 'min_samples_split': 13, 'min_samples_leaf': 6, 'subsample': 0.8778074320789581}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:40:34,133] Trial 45 finished with value: 0.8886054421768709 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.14630909145263746, 'min_samples_split': 13, 'min_samples_leaf': 6, 'subsample': 0.7076158550785229}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:49:42,682] Trial 46 finished with value: 0.8895502645502644 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.10581121630555594, 'min_samples_split': 13, 'min_samples_leaf': 6, 'subsample': 0.7184929738075713}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:50:52,896] Trial 47 finished with value: 0.8900226757369616 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.10457160112321355, 'min_samples_split': 13, 'min_samples_leaf': 6, 'subsample': 0.7043591868957079}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:52:30,787] Trial 48 finished with value: 0.8881330309901738 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.10672587782092031, 'min_samples_split': 13, 'min_samples_leaf': 6, 'subsample': 0.996830770968461}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:54:49,285] Trial 52 finished with value: 0.8855820105820105 and parameters: {'n_estimators': 300, 'max_depth': 12, 'learning_rate': 0.10658121229625872, 'min_samples_split': 13, 'min_samples_leaf': 2, 'subsample': 0.7154408650217388}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:55:39,920] Trial 50 finished with value: 0.8889833711262283 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.09439151648674415, 'min_samples_split': 13, 'min_samples_leaf': 6, 'subsample': 0.7094937481762554}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:56:25,654] Trial 49 finished with value: 0.8869992441421013 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.09306364982053864, 'min_samples_split': 13, 'min_samples_leaf': 6, 'subsample': 0.9890934763223735}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 14:57:16,088] Trial 51 finished with value: 0.8872826908541195 and parameters: {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.10524205133122827, 'min_samples_split': 13, 'min_samples_leaf': 6, 'subsample': 0.9894635846889517}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 15:01:58,235] Trial 53 finished with value: 0.886432350718065 and parameters: {'n_estimators': 300, 'max_depth': 12, 'learning_rate': 0.10745233198474186, 'min_samples_split': 13, 'min_samples_leaf': 2, 'subsample': 0.9900359213755676}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 15:08:05,062] Trial 57 finished with value: 0.8912509448223733 and parameters: {'n_estimators': 450, 'max_depth': 16, 'learning_rate': 0.1261901704278515, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.7625188438534565}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 15:08:20,328] Trial 54 finished with value: 0.8858654572940287 and parameters: {'n_estimators': 300, 'max_depth': 12, 'learning_rate': 0.09224380731437047, 'min_samples_split': 13, 'min_samples_leaf': 2, 'subsample': 0.9934107732484172}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 15:10:29,130] Trial 55 finished with value: 0.8859599395313681 and parameters: {'n_estimators': 450, 'max_depth': 12, 'learning_rate': 0.09394952156025149, 'min_samples_split': 13, 'min_samples_leaf': 2, 'subsample': 0.8968255334212283}. Best is trial 29 with value: 0.8921012849584279.\n",
      "[I 2024-12-07 15:10:56,953] Trial 56 finished with value: 0.8866213151927438 and parameters: {'n_estimators': 450, 'max_depth': 12, 'learning_rate': 0.09171576976193628, 'min_samples_split': 5, 'min_samples_leaf': 2, 'subsample': 0.7632129875350455}. Best is trial 29 with value: 0.8921012849584279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8921012849584279\n",
      "Meilleurs paramètres trouvés : {'n_estimators': 300, 'max_depth': 16, 'learning_rate': 0.11782851482424293, 'min_samples_split': 5, 'min_samples_leaf': 6, 'subsample': 0.7409699669018692}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fonction objectif pour Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [ 300, 400, 450, 500]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [ 10, 12, 14, 16]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.15),\n",
    "        'min_samples_split': trial.suggest_categorical('min_samples_split', [ 5, 10, 13]),\n",
    "        'min_samples_leaf': trial.suggest_categorical('min_samples_leaf', [ 2, 4, 6]),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = GradientBoostingClassifier(**params)\n",
    "    score = cross_val_score(model, X_train_hill, y_train, cv=7, scoring='accuracy').mean()\n",
    "    return score\n",
    "\n",
    "# Lancer l'optimisation avec Optuna\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"gb_optimization\", storage=\"sqlite:///optuna_study.db\", load_if_exists=True)\n",
    "study.optimize(objective, n_trials=50, n_jobs=-1)  # Parallélisation avec n_jobs=-1\n",
    "\n",
    "# Afficher les meilleurs paramètres\n",
    "print(\"Best score:\", study.best_value)\n",
    "print(\"Meilleurs paramètres trouvés :\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Définir les modèles de base avec les meilleurs hyperparamètres\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=300,  \n",
    "    max_depth=None,      \n",
    "    random_state=42,\n",
    "    max_features='sqrt',\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    ")\n",
    "\n",
    "xgboost = XGBClassifier(\n",
    "    colsample_bytree=0.9935889419671114,\n",
    "    gamma=0.012860005064671678,\n",
    "    learning_rate=0.11209385123371478,\n",
    "    max_depth=9,\n",
    "    n_estimators=200,\n",
    "    subsample=0.7078887451962781,\n",
    "    objective='multi:softmax',\n",
    "    num_class=7,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lightgbm = LGBMClassifier(\n",
    "    colsample_bytree=0.6284626649933062,\n",
    "    learning_rate=0.08427513495693706,\n",
    "    max_depth=15,\n",
    "    n_estimators=200,\n",
    "    num_leaves=100,\n",
    "    subsample=0.6894988308344379,\n",
    "    objective='multiclass',\n",
    "    num_class=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "extra_trees = ExtraTreesClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    max_features=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "gradient_boosting = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.11782851482424293,\n",
    "    max_depth=16,\n",
    "    random_state=42,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=6, \n",
    "    subsample=0.7409699669018692 \n",
    ")\n",
    "\n",
    "# Définir le méta-modèle (un modèle simple comme la régression logistique)\n",
    "meta_model = XGBClassifier(\n",
    "    n_estimators=160, #150 #200,\n",
    "    max_depth= 2,  #3,#4\n",
    "    learning_rate= 0.1,  #0.1,\n",
    "    colsample_bytree=0.92,\n",
    "    subsample=0.75,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "    # LGBMClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "    # RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42)\n",
    "    # LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Créer le StackingClassifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', random_forest),\n",
    "        ('xgb', xgboost),\n",
    "        ('lgbm', lightgbm),\n",
    "        ('et', extra_trees),\n",
    "        ('gb', gradient_boosting)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=10  # Utiliser une validation croisée pour le stacking\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steph/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [17:00:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle de stacking : 0.8996913580246914\n",
      "Rapport de classification du modèle de stacking :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       648\n",
      "           1       0.80      0.80      0.80       648\n",
      "           2       0.90      0.91      0.90       648\n",
      "           3       0.97      0.97      0.97       648\n",
      "           4       0.95      0.94      0.95       648\n",
      "           5       0.91      0.93      0.92       648\n",
      "           6       0.96      0.94      0.95       648\n",
      "\n",
      "    accuracy                           0.90      4536\n",
      "   macro avg       0.90      0.90      0.90      4536\n",
      "weighted avg       0.90      0.90      0.90      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entraîner le modèle de stacking\n",
    "stacking_model.fit(X_train_hill, y_train_adj)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred_stack = stacking_model.predict(X_test_hill)\n",
    "\n",
    "# Évaluer les performances du modèle de stacking\n",
    "accuracy_stack = accuracy_score(y_test_adj, y_pred_stack)\n",
    "print(\"Précision du modèle de stacking :\", accuracy_stack)\n",
    "print(\"Rapport de classification du modèle de stacking :\\n\", classification_report(y_test_adj, y_pred_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_adj = y - 1\n",
    "\n",
    "X_test_full = pd.read_csv('test-full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Hillshade_9am'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHillshade_9am\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m X_test_full \u001b[38;5;241m=\u001b[39m X_test_full\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHillshade_9am\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5589\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Hillshade_9am'] not found in axis\""
     ]
    }
   ],
   "source": [
    "X = X.drop(columns=['Hillshade_9am'])\n",
    "X_test_full = X_test_full.drop(columns=['Hillshade_9am'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steph/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [15:08:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# On entraîne le modèle de stacking\n",
    "stacking_model.fit(X, y_adj)\n",
    "\n",
    "# On prédit sur l'ensemble de test\n",
    "y_pred_stack = stacking_model.predict(X_test_full)\n",
    "\n",
    "y_pred_stack = y_pred_stack + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée un DataFrame avec les Id et les Cover_Type prédits\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': X_test_full['Id'],\n",
    "    'Cover_Type': y_pred_stack\n",
    "})\n",
    "\n",
    "# On sauvegarde le DataFrame en fichier CSV\n",
    "submission_df.to_csv('soumissions/submission_stacking_Hillshade_opt_170.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
