{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation hyperparamètres des modèles avec le features engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va optimiser les hyperparamètres des modèles prometteurs que nous avons identifiés dans le notebook précédent. \n",
    "\n",
    "On va utiliser des méthodes de recherche d'hyperparamètres par validation croisée pour éviter le surapprentissage.\n",
    "\n",
    "On va utiliser les méthodes bayésiennes pour optimiser les hyperparamètres des modèles de manière intelligente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# On sélectionne les colonnes d'entrée (features) et la cible\n",
    "X = data.drop(columns=['Cover_Type'])\n",
    "y = data['Cover_Type']\n",
    "\n",
    "# On sélectionne les colonnes d'entrée (features) et la cible\n",
    "X = data.drop(columns=['Cover_Type'])\n",
    "y = data['Cover_Type']\n",
    "\n",
    "# On divise les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# On normalise les données continues \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "continuous_columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', \n",
    "                      'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', \n",
    "                      'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# On applique le scaler uniquement sur les colonnes continues\n",
    "#X_train[continuous_columns] = scaler.fit_transform(X_train[continuous_columns])\n",
    "#X_test[continuous_columns] = scaler.transform(X_test[continuous_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire des types de sols avec leurs codes ELU\n",
    "soil_type_to_elu = {\n",
    "    1: \"2702\",  2: \"2703\",  3: \"2704\",  4: \"2705\",  5: \"2706\",  6: \"2717\",\n",
    "    7: \"3501\",  8: \"3502\",  9: \"4201\", 10: \"4703\", 11: \"4704\", 12: \"4744\",\n",
    "    13: \"4758\", 14: \"5101\", 15: \"5151\", 16: \"6101\", 17: \"6102\", 18: \"6731\",\n",
    "    19: \"7101\", 20: \"7102\", 21: \"7103\", 22: \"7201\", 23: \"7202\", 24: \"7700\",\n",
    "    25: \"7701\", 26: \"7702\", 27: \"7709\", 28: \"7710\", 29: \"7745\", 30: \"7746\",\n",
    "    31: \"7755\", 32: \"7756\", 33: \"7757\", 34: \"7790\", 35: \"8703\", 36: \"8707\",\n",
    "    37: \"8708\", 38: \"8771\", 39: \"8772\", 40: \"8776\"\n",
    "}\n",
    "\n",
    "# Fonction pour extraire les composantes du code ELU\n",
    "def extract_elu_components(elu_code):\n",
    "    climatic_zone = int(elu_code[0])  # Premier chiffre pour la zone climatique\n",
    "    geologic_zone = int(elu_code[1])  # Deuxième chiffre pour la zone géologique\n",
    "    return climatic_zone, geologic_zone\n",
    "\n",
    "# Fonction pour ajouter les colonnes climatic_zone et geologic_zone\n",
    "def add_climatic_geologic_zones(df, soil_type_to_elu):\n",
    "    # Faire une copie explicite du DataFrame pour éviter les effets de référence\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Initialiser des listes pour stocker les valeurs des zones climatiques et géologiques\n",
    "    climatic_zones = []\n",
    "    geologic_zones = []\n",
    "\n",
    "    # Parcourir chaque ligne pour identifier le type de sol\n",
    "    for i, row in df.iterrows():\n",
    "        # Filtrer les colonnes `Soil_TypeX` pour cette ligne\n",
    "        soil_type_columns = [col for col in df.columns if 'Soil_Type' in col]\n",
    "        soil_type_values = row[soil_type_columns]\n",
    "        \n",
    "        # Vérifier si une seule colonne `Soil_TypeX` a la valeur 1\n",
    "        if soil_type_values.sum() == 1:\n",
    "            # Identifier le type de sol en trouvant la colonne avec la valeur 1\n",
    "            soil_type = int(soil_type_values.idxmax().split('Soil_Type')[1])\n",
    "            \n",
    "            # Récupérer le code ELU pour le type de sol\n",
    "            elu_code = soil_type_to_elu.get(soil_type)\n",
    "            \n",
    "            # Vérification supplémentaire : s'assurer que le code ELU existe\n",
    "            if elu_code:\n",
    "                # Extraire la zone climatique et géologique à partir du code ELU\n",
    "                climatic_zone, geologic_zone = extract_elu_components(elu_code)\n",
    "            else:\n",
    "                # Si le code ELU est manquant, utiliser -1\n",
    "                climatic_zone, geologic_zone = -1, -1\n",
    "        else:\n",
    "            # Si aucune colonne ou plusieurs colonnes sont égales à 1, utiliser -1\n",
    "            climatic_zone, geologic_zone = -1, -1\n",
    "\n",
    "        # Ajouter les valeurs aux listes\n",
    "        climatic_zones.append(climatic_zone)\n",
    "        geologic_zones.append(geologic_zone)\n",
    "\n",
    "    # Vérifier que les longueurs des listes correspondent au nombre de lignes dans df\n",
    "    assert len(climatic_zones) == len(df), \"Erreur : La longueur des listes ne correspond pas au nombre de lignes du DataFrame\"\n",
    "\n",
    "    # Ajouter les nouvelles colonnes au DataFrame en utilisant `assign`\n",
    "    df = df.assign(\n",
    "        climatic_zone=pd.Series(climatic_zones, index=df.index, dtype=\"float\"),\n",
    "        geologic_zone=pd.Series(geologic_zones, index=df.index, dtype=\"float\")\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de NaN dans chaque colonne :\n",
      "climatic_zone    0\n",
      "geologic_zone    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train_soil = add_climatic_geologic_zones(X_train, soil_type_to_elu)\n",
    "\n",
    "# Vérifier le nombre de NaN dans les colonnes `climatic_zone` et `geologic_zone`\n",
    "nan_counts = X_train_soil[['climatic_zone', 'geologic_zone']].isna().sum()\n",
    "print(\"Nombre de NaN dans chaque colonne :\")\n",
    "print(nan_counts)\n",
    "\n",
    "X_test_soil = add_climatic_geologic_zones(X_test, soil_type_to_elu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [17:24<00:00, 20.88s/trial, best loss: -0.8731103552532125]\n",
      "Meilleurs paramètres Random Forest : {'max_depth': 4, 'max_features': 0, 'min_samples_leaf': 0, 'min_samples_split': 0, 'n_estimators': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# On définit l'espace de recherche pour Random Forest\n",
    "space_rf = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [5, 10, 15, 20, None]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4]),\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'log2'])  # Correction : 'auto' remplacé par 'sqrt'\n",
    "}\n",
    "\n",
    "# On crée la fonction objectif pour Random Forest\n",
    "def objective_rf(params):\n",
    "    # On initialise le modèle avec les paramètres proposés\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    # On effectue une validation croisée pour évaluer la performance\n",
    "    score = cross_val_score(model, X_train_soil, y_train, cv=8, scoring='accuracy').mean()\n",
    "    # On retourne la perte (score négatif) car Hyperopt minimise cette fonction\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# On lance Hyperopt pour optimiser les hyperparamètres de Random Forest\n",
    "trials_rf = Trials()\n",
    "best_params_rf = fmin(fn=objective_rf, space=space_rf, algo=tpe.suggest, max_evals=50, trials=trials_rf)\n",
    "print(\"Meilleurs paramètres Random Forest :\", best_params_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste ensuite le modèle avec les hyperparamètres optimisés sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision Random Forest : 0.8761022927689595\n",
      "Rapport de classification Random Forest :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.80      0.80       648\n",
      "           2       0.81      0.71      0.76       648\n",
      "           3       0.84      0.88      0.86       648\n",
      "           4       0.95      0.97      0.96       648\n",
      "           5       0.92      0.95      0.93       648\n",
      "           6       0.87      0.87      0.87       648\n",
      "           7       0.94      0.95      0.94       648\n",
      "\n",
      "    accuracy                           0.88      4536\n",
      "   macro avg       0.87      0.88      0.87      4536\n",
      "weighted avg       0.87      0.88      0.87      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Traduction des indices en valeurs concrètes\n",
    "best_rf_params = {\n",
    "    'max_depth': [5, 10, 15, 20, None][4],           \n",
    "    'max_features': ['sqrt', 'log2'][0],             \n",
    "    'min_samples_leaf': [1, 2, 4][0],                \n",
    "    'min_samples_split': [2, 5, 10][0],              \n",
    "    'n_estimators': [100, 200, 300, 400, 500][4]     \n",
    "}\n",
    "\n",
    "# Initialiser le modèle avec les valeurs concrètes\n",
    "rf_model = RandomForestClassifier(**best_rf_params, random_state=42)\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "rf_model.fit(X_train_soil, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "y_pred_rf = rf_model.predict(X_test_soil)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Précision Random Forest :\", accuracy_rf)\n",
    "print(\"Rapport de classification Random Forest :\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:27:23<00:00, 104.86s/trial, best loss: -0.8877551020408163]  \n",
      "Meilleurs paramètres XGBoost : {'colsample_bytree': 0.5648136746434348, 'gamma': 0.015587699613612005, 'learning_rate': 0.11668163771105296, 'max_depth': 3, 'n_estimators': 3, 'subsample': 0.9610642479341145}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_adj = y_train - 1\n",
    "y_test_adj = y_test - 1\n",
    "\n",
    "# On définit l'espace de recherche pour XGBoost\n",
    "space_xgb = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [3, 6, 9, 12]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0, 0.5)\n",
    "}\n",
    "\n",
    "# On crée la fonction objectif pour XGBoost\n",
    "def objective_xgb(params):\n",
    "    # On initialise XGBoost avec les paramètres proposés\n",
    "    model = xgb.XGBClassifier(objective='multi:softmax', num_class=7, **params, random_state=42)\n",
    "    # On utilise une validation croisée pour calculer la précision\n",
    "    score = cross_val_score(model, X_train_soil, y_train_adj, cv=8, scoring='accuracy').mean()\n",
    "    # On retourne la perte (score négatif) car Hyperopt minimise cette fonction\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# On lance l'optimisation avec Hyperopt pour XGBoost\n",
    "trials_xgb = Trials()\n",
    "best_params_xgb = fmin(fn=objective_xgb, space=space_xgb, algo=tpe.suggest, max_evals=50, trials=trials_xgb)\n",
    "print(\"Meilleurs paramètres XGBoost :\", best_params_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste ensuite le modèle avec les hyperparamètres optimisés sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle XGBoost : 0.8910934744268078\n",
      "Rapport de classification XGBoost :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80       648\n",
      "           1       0.81      0.75      0.78       648\n",
      "           2       0.89      0.90      0.89       648\n",
      "           3       0.96      0.98      0.97       648\n",
      "           4       0.92      0.96      0.94       648\n",
      "           5       0.90      0.91      0.91       648\n",
      "           6       0.94      0.95      0.95       648\n",
      "\n",
      "    accuracy                           0.89      4536\n",
      "   macro avg       0.89      0.89      0.89      4536\n",
      "weighted avg       0.89      0.89      0.89      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On utilise les meilleurs hyperparamètres trouvés par Hyperopt\n",
    "best_xgb_params = {\n",
    "    'colsample_bytree': 0.5648136746434348,\n",
    "    'gamma': 0.015587699613612005,\n",
    "    'learning_rate': 0.11668163771105296,\n",
    "    'max_depth': [3, 6, 9, 12][3],\n",
    "    'n_estimators': [100, 200, 300, 400, 500][3],  \n",
    "    'subsample': 0.9610642479341145\n",
    "}\n",
    "\n",
    "# Initialiser le modèle XGBoost avec les paramètres optimaux\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=7, **best_xgb_params, random_state=42)\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "xgb_model.fit(X_train_soil, y_train_adj)\n",
    "\n",
    "# Prédire les classes sur l'ensemble de test\n",
    "y_pred_xgb = xgb_model.predict(X_test_soil)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy_xgb = accuracy_score(y_test_adj, y_pred_xgb)\n",
    "print(\"Précision du modèle XGBoost :\", accuracy_xgb)\n",
    "print(\"Rapport de classification XGBoost :\\n\", classification_report(y_test_adj, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:03:07<00:00, 75.74s/trial, best loss: -0.8921012849584278]\n",
      "Meilleurs paramètres LightGBM : {'colsample_bytree': 0.6006084259672398, 'learning_rate': 0.037177160869918895, 'max_depth': 3, 'n_estimators': 3, 'num_leaves': 3, 'subsample': 0.811448630394428}\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# On définit l'espace de recherche pour LightGBM\n",
    "space_lgb = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [5, 10, 15, 20, -1]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'num_leaves': hp.choice('num_leaves', [31, 50, 70, 100]),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0)\n",
    "}\n",
    "\n",
    "# On crée la fonction objectif pour LightGBM\n",
    "def objective_lgb(params):\n",
    "    # On initialise LightGBM avec les paramètres proposés\n",
    "    model = lgb.LGBMClassifier(objective='multiclass', num_class=7,**params, verbosity = -1,random_state=42)\n",
    "    # On utilise la validation croisée pour évaluer la précision\n",
    "    score = cross_val_score(model, X_train_soil, y_train, cv=8, scoring='accuracy').mean()\n",
    "    # On retourne la perte (score négatif) car Hyperopt minimise cette fonction\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# On lance l'optimisation avec Hyperopt pour LightGBM\n",
    "trials_lgb = Trials()\n",
    "best_params_lgb = fmin(fn=objective_lgb, space=space_lgb, algo=tpe.suggest, max_evals=50, trials=trials_lgb)\n",
    "print(\"Meilleurs paramètres LightGBM :\", best_params_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste ensuite le modèle avec les hyperparamètres optimisés sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle LightGBM : 0.8919753086419753\n",
      "Rapport de classification LightGBM :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.79      0.80       648\n",
      "           2       0.81      0.75      0.77       648\n",
      "           3       0.89      0.90      0.90       648\n",
      "           4       0.96      0.98      0.97       648\n",
      "           5       0.93      0.96      0.94       648\n",
      "           6       0.90      0.91      0.91       648\n",
      "           7       0.94      0.95      0.95       648\n",
      "\n",
      "    accuracy                           0.89      4536\n",
      "   macro avg       0.89      0.89      0.89      4536\n",
      "weighted avg       0.89      0.89      0.89      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Meilleurs paramètres de LightGBM (après avoir traduit les indices en valeurs)\n",
    "best_lgb_params = {\n",
    "    'colsample_bytree': 0.6006084259672398,\n",
    "    'learning_rate': 0.037177160869918895,\n",
    "    'max_depth': [5, 10, 15, 20, -1][3],\n",
    "    'n_estimators': [100, 200, 300, 400, 500][3],  \n",
    "    'num_leaves': [31, 50, 70, 100][3],             \n",
    "    'subsample': 0.811448630394428,\n",
    "    'verbosity': -1  # Désactive les avertissements pendant l'entraînement\n",
    "}\n",
    "\n",
    "# Initialiser le modèle LightGBM avec les paramètres optimisés\n",
    "lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=7, **best_lgb_params, random_state=42)\n",
    "\n",
    "# Entraîner le modèle sur l'ensemble d'entraînement\n",
    "lgb_model.fit(X_train_soil, y_train)\n",
    "\n",
    "# Prédire les classes sur l'ensemble de test\n",
    "y_pred_lgb = lgb_model.predict(X_test_soil)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "print(\"Précision du modèle LightGBM :\", accuracy_lgb)\n",
    "print(\"Rapport de classification LightGBM :\\n\", classification_report(y_test, y_pred_lgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Définir les modèles de base avec les meilleurs hyperparamètres\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=500,  \n",
    "    max_depth=None,      \n",
    "    random_state=42,\n",
    "    max_features='sqrt',\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    ")\n",
    "\n",
    "xgboost = xgb.XGBClassifier(\n",
    "    colsample_bytree=0.5648136746434348,\n",
    "    gamma=0.015587699613612005,\n",
    "    learning_rate=0.11668163771105296,\n",
    "    max_depth=12,\n",
    "    n_estimators=400,\n",
    "    subsample=0.9610642479341145,\n",
    "    objective='multi:softmax',\n",
    "    num_class=7,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lightgbm = lgb.LGBMClassifier(\n",
    "    colsample_bytree=0.6006084259672398,\n",
    "    learning_rate=0.037177160869918895,\n",
    "    max_depth=20,\n",
    "    n_estimators=400,\n",
    "    num_leaves=100,\n",
    "    subsample=0.811448630394428,\n",
    "    objective='multiclass',\n",
    "    num_class=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "# Définir le méta-modèle (un modèle simple comme la régression logistique)\n",
    "meta_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Créer le StackingClassifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', random_forest),\n",
    "        ('xgb', xgboost),\n",
    "        ('lgbm', lightgbm)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=10  # Utiliser une validation croisée pour le stacking\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle de stacking : 0.8893298059964727\n",
      "Rapport de classification du modèle de stacking :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80       648\n",
      "           1       0.79      0.75      0.77       648\n",
      "           2       0.89      0.91      0.90       648\n",
      "           3       0.96      0.98      0.97       648\n",
      "           4       0.94      0.94      0.94       648\n",
      "           5       0.90      0.91      0.90       648\n",
      "           6       0.95      0.94      0.95       648\n",
      "\n",
      "    accuracy                           0.89      4536\n",
      "   macro avg       0.89      0.89      0.89      4536\n",
      "weighted avg       0.89      0.89      0.89      4536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entraîner le modèle de stacking\n",
    "stacking_model.fit(X_train_soil, y_train_adj)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "X_test_soil = add_climatic_geologic_zones(X_test, soil_type_to_elu)\n",
    "y_pred_stack = stacking_model.predict(X_test_soil)\n",
    "\n",
    "# Évaluer les performances du modèle de stacking\n",
    "accuracy_stack = accuracy_score(y_test_adj, y_pred_stack)\n",
    "print(\"Précision du modèle de stacking :\", accuracy_stack)\n",
    "print(\"Rapport de classification du modèle de stacking :\\n\", classification_report(y_test_adj, y_pred_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_soil = add_climatic_geologic_zones(X, soil_type_to_elu)\n",
    "y_adj = y - 1\n",
    "\n",
    "\n",
    "X_test_full = pd.read_csv('test-full.csv')\n",
    "X_test_full_soil = add_climatic_geologic_zones(X_test_full, soil_type_to_elu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On entraîne le modèle de stacking\n",
    "stacking_model.fit(X_soil, y_adj)\n",
    "\n",
    "# On prédit sur l'ensemble de test\n",
    "y_pred_stack = stacking_model.predict(X_test_full_soil)\n",
    "\n",
    "y_pred_stack = y_pred_stack + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée un DataFrame avec les Id et les Cover_Type prédits\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': X_test_full['Id'],\n",
    "    'Cover_Type': y_pred_stack\n",
    "})\n",
    "\n",
    "# On sauvegarde le DataFrame en fichier CSV\n",
    "submission_df.to_csv('soumissions/submission_soil_opt10.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
